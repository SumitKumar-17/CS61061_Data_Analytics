# -*- coding: utf-8 -*-
"""Data_Analytics_A2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RsQJGg9JiyNChJxWGpIgPrhOKHboPtl4
"""

# prompt: load the dataset into a df
from itertools import combinations
from collections import defaultdict

import pandas as pd

# Assuming your dataset is in a CSV file named 'your_dataset.csv'
df = pd.read_csv('groceries.csv')

# Print the first 5 rows of the dataframe to verify it loaded correctly
df.head()

df.isnull().sum()

# Preprocess the dataset to create a list of transactions (sets of items)
transactions = [set(items.split(',')) for items in df['Items'].dropna()]

# Check the transactions list to confirm preprocessing
print("Sample transactions:", transactions[:5])  # Display the first few transactions for verification

# prompt: find the unique values among all the entries in df and tell their number

from collections import defaultdict

unique_items = set()
for transaction in transactions:
  for item in transaction:
    unique_items.add(item)

print("Unique items:", unique_items)
print("Number of unique items:", len(unique_items))

# Create a dictionary to store the one-hot encoded data
one_hot_data = defaultdict(list)

# Iterate through each transaction
for transaction in transactions:
  # Iterate through each unique item
  for item in unique_items:
    # If the item is present in the current transaction, add 1, otherwise add 0
    if item in transaction:
      one_hot_data[item].append(1)
    else:
      one_hot_data[item].append(0)

# Create a new DataFrame from the one-hot encoded data
one_hot_df = pd.DataFrame(one_hot_data)

# Print the one-hot encoded DataFrame
one_hot_df

# Calculate the number of 1s in each row
row_sums = one_hot_df.sum(axis=1)

# Print the results
print("Number of 1s in each row:\n", row_sums)

def func(data):
  for i in unique_items:
    if data[i] > 0:
      data[i] = i
  return data

data2 = one_hot_df.apply(func,axis=1)

data2.head()

newdata = data2.values

newdata

newdata.ndim

newdata = [i[i!=0].tolist() for i in newdata if i[i!=0].tolist()]

newdata[:10]

def generate_itemsets(transactions, min_support):
    """Generate frequent itemsets using the Apriori algorithm."""
    item_counts = defaultdict(int)
    transaction_count = len(transactions)

    # Count single items
    for transaction in transactions:
        for item in transaction:
            item_counts[frozenset([item])] += 1

    # Filter single items by min_support
    current_itemsets = {itemset: count for itemset, count in item_counts.items()
                        if count / transaction_count >= min_support}

    # Store frequent itemsets
    frequent_itemsets = {}
    frequent_itemsets.update(current_itemsets)

    # Generate itemsets of size k > 1
    k = 2
    while current_itemsets:
        candidate_itemsets = defaultdict(int)
        itemsets_list = list(current_itemsets.keys())

        # Generate candidate itemsets of size k by joining pairs of size k-1 itemsets
        for i in range(len(itemsets_list)):
            for j in range(i + 1, len(itemsets_list)):
                candidate = itemsets_list[i] | itemsets_list[j]
                if len(candidate) == k:
                    # Count support for candidate itemset
                    candidate_count = sum(1 for transaction in transactions if candidate.issubset(transaction))
                    if candidate_count / transaction_count >= min_support:
                        candidate_itemsets[candidate] += candidate_count

        # Update frequent itemsets
        frequent_itemsets.update({itemset: count for itemset, count in candidate_itemsets.items() if count > 0})
        current_itemsets = candidate_itemsets
        k += 1

    return frequent_itemsets

def generate_association_rules(frequent_itemsets, min_confidence):
    """Generate association rules from frequent itemsets."""
    rules = []

    for itemset, support_count in frequent_itemsets.items():
        # Generate all non-empty subsets of the itemset
        subsets = [frozenset(x) for i in range(1, len(itemset)) for x in combinations(itemset, i)]
        for subset in subsets:
            # Calculate the consequent
            consequent = itemset - subset

            if len(consequent) > 0:
                # Calculate support, confidence, and lift
                support = support_count
                support_consequent = sum(1 for k, v in frequent_itemsets.items() if consequent.issubset(k) and v > 0)
                confidence = support / support_consequent if support_consequent > 0 else 0
                lift = confidence / (support_consequent / len(frequent_itemsets))

                if confidence >= min_confidence:
                    rules.append((set(subset), set(consequent), support, confidence, lift))

    return rules

# Input transactions
transactions = newdata
transactions

# Input minimum support and generate frequent itemsets
min_support = float(input("Enter the minimum support (e.g., 0.01): "))
frequent_itemsets = generate_itemsets(transactions, min_support)

# Print frequent itemsets and their support values
print("Frequent Itemsets and Support Values:")
for itemset, support_count in frequent_itemsets.items():
    support_value = support_count / len(transactions)  # Calculate support value
    print(f"{set(itemset)}: Support Value: {support_value:.4f}")

# Input minimum confidence and generate association rules
min_confidence = float(input("Enter the minimum confidence (e.g., 0.5): "))
rules = generate_association_rules(frequent_itemsets, min_confidence)

# Print association rules with their metrics
print("Association Rules:")
for rule in rules:
    antecedent, consequent, support, confidence, lift = rule
    print(f"Rule: {antecedent} -> {consequent}, Support: {support/len(transactions):.4f}, Confidence: {confidence:.4f}, Lift: {lift:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
def plot_top_n_frequent_itemsets(frequent_itemsets, n=5):
    """Visualize the top-N frequent itemsets using a bar chart."""
    # Sort frequent itemsets by support count
    sorted_itemsets = sorted(frequent_itemsets.items(), key=lambda x: x[1], reverse=True)

    # Get top N itemsets
    top_n_itemsets = sorted_itemsets[:n]

    # Prepare data for plotting
    itemset_labels = [str(set(itemset)) for itemset, _ in top_n_itemsets]
    support_values = [count for _, count in top_n_itemsets]

    # Create bar chart
    plt.figure(figsize=(10, 6))
    sns.barplot(x=support_values, y=itemset_labels, palette='viridis')
    plt.title(f'Top-{n} Frequent Itemsets')
    plt.xlabel('Support Count')
    plt.ylabel('Itemsets')
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def plot_top_n_strongest_rules(rules, n=5):
    """Visualize the top-N strongest rules using a scatter plot with jitter and print rule details."""
    # Sort rules by lift and select the top-N
    sorted_rules = sorted(rules, key=lambda x: x[4], reverse=True)
    top_n_rules = sorted_rules[:n]

    # Prepare data for plotting
    antecedents = [' & '.join(map(str, rule[0])) for rule in top_n_rules]
    consequents = [' & '.join(map(str, rule[1])) for rule in top_n_rules]
    supports = [rule[2] / len(transactions) for rule in top_n_rules]  # Normalized support
    confidences = [rule[3] for rule in top_n_rules]
    lifts = np.array([rule[4] for rule in top_n_rules])

    # Add a small jitter to prevent overlapping points
    supports_jittered = np.array(supports) + np.random.normal(0, 0.000001, size=len(supports))
    lifts_jittered = lifts + np.random.normal(0, 0.000001, size=len(lifts))

    # Create scatter plot with uniform mark points
    plt.figure(figsize=(20, 10))
    sns.scatterplot(x=lifts_jittered, y=supports_jittered, s=100, marker='o', color='b', alpha=0.6)

    # Annotate points with rule details
    for i in range(len(top_n_rules)):
        plt.annotate(f"{antecedents[i]} -> {consequents[i]}", (lifts_jittered[i], supports_jittered[i]), fontsize=9, ha='right')

    plt.title(f'Top-{n} Strongest Rules (by Lift)')
    plt.xlabel('Lift')
    plt.ylabel('Normalized Support')
    plt.grid()
    plt.show()

    # Print rule details separately
    print(f"Top {n} Strongest Rules:")
    print(f"{'Antecedent':<50} {'Consequent':<50} {'Support':<10} {'Confidence':<10} {'Lift':<10}")
    print("="*120)
    for i in range(n):
        print(f"{antecedents[i]:<50} {consequents[i]:<50} {supports[i]:<10.6f} {confidences[i]:<10.4f} {lifts[i]:<10.4f}")

def summarize_results(frequent_itemsets, rules):
    """Generate a detailed report summarizing the frequent patterns and association rules."""
    print("\nFrequent Itemsets Summary:")
    for itemset, count in frequent_itemsets.items():
        support_value = count / len(transactions)
        print(f"Itemset: {set(itemset)}, Support Count: {count}, Support Value: {support_value:.4f}")

    print("\nAssociation Rules Summary:")
    for rule in rules:
        antecedent, consequent, support, confidence, lift = rule
        print(f"Rule: {antecedent} -> {consequent}, Support: {support/len(transactions):.6f}, Confidence: {confidence:.4f}, Lift: {lift:.4f}")


"""# Bonus Part"""

import itertools

def generate_association_rules(frequent_itemsets, transactions, min_confidence, min_leverage=None, min_conviction=None):
    """Generate association rules with support, confidence, lift, leverage, and conviction."""
    rules = []
    transaction_count = len(transactions)

    # Calculate support for individual items
    item_supports = {itemset: support / transaction_count for itemset, support in frequent_itemsets.items()}

    # Generate rules for each frequent itemset
    for itemset in frequent_itemsets:
        if len(itemset) < 2:
            continue  # Skip if itemset cannot form a rule

        for antecedent in itertools.chain.from_iterable(itertools.combinations(itemset, r) for r in range(1, len(itemset))):
            antecedent = frozenset(antecedent)
            consequent = itemset - antecedent

            if consequent:
                antecedent_support = item_supports[antecedent]
                consequent_support = item_supports[consequent]
                rule_support = frequent_itemsets[itemset] / transaction_count
                confidence = rule_support / antecedent_support
                lift = confidence / consequent_support

                # Calculate leverage and conviction
                leverage = rule_support - (antecedent_support * consequent_support)
                # Avoid infinity for conviction by setting a high value if confidence is 1
                conviction = ((1 - consequent_support) / (1 - confidence)) if confidence < 1 else 10**6

                # Apply minimum confidence and additional thresholds if specified
                if confidence >= min_confidence:
                    if (min_leverage is None or leverage >= min_leverage) and \
                       (min_conviction is None or conviction >= min_conviction):
                        rules.append((antecedent, consequent, rule_support, confidence, lift, leverage, conviction))

    return rules

# Example usage
# Assume 'frequent_itemsets' is a dictionary containing itemsets as keys and their support counts as values
# Assume 'transactions' is a list of transactions (each transaction is a set of items)

# User inputs
min_confidence = float(input("Enter minimum confidence (e.g., 0.1): "))
min_leverage = float(input("Enter minimum leverage (e.g., 0.005) or leave blank: ") or 0.01)
min_conviction = float(input("Enter minimum conviction (e.g., 0.8) or leave blank: ") or 1.2)

# Generate association rules with specified thresholds
rules = generate_association_rules(frequent_itemsets, transactions, min_confidence, min_leverage, min_conviction)

# Print the generated rules
print("\nGenerated Association Rules with Leverage and Conviction:")
for rule in rules:
    antecedent, consequent, support, confidence, lift, leverage, conviction = rule
    print(f"Rule: {set(antecedent)} -> {set(consequent)}")
    print(f"  Support: {support:.4f}, Confidence: {confidence:.4f}, Lift: {lift:.4f}, Leverage: {leverage:.4f}, Conviction: {conviction:.4f}")
    print("="*60)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def plot_top_n_strongest_rules(rules, transactions, n=5):
    """Visualize the top-N strongest rules using a scatter plot with jitter to avoid overlapping points and print details."""
    # Sort rules by lift and select the top-N
    sorted_rules = sorted(rules, key=lambda x: x[4], reverse=True)[:n]  # x[4] is lift
    antecedents = [' & '.join(map(str, rule[0])) for rule in sorted_rules]
    consequents = [' & '.join(map(str, rule[1])) for rule in sorted_rules]
    lifts = [rule[4] for rule in sorted_rules]
    leverages = [rule[5] for rule in sorted_rules]
    convictions = [rule[6] for rule in sorted_rules]

    # Add a small jitter to lift and leverage to avoid overlapping
    lifts_jittered = np.array(lifts) + np.random.normal(0, 0.0005, size=len(lifts))
    leverages_jittered = np.array(leverages) + np.random.normal(0, 0.0005, size=len(leverages))

    # Plotting
    plt.figure(figsize=(14, 8))
    sns.scatterplot(x=lifts_jittered, y=leverages_jittered, s=200, alpha=0.7, marker='o', color='blue')  # Fixed marker symbol
    plt.xlabel("Lift")
    plt.ylabel("Leverage")
    plt.title(f"Top-{n} Strongest Rules by Lift (with Leverage and Conviction)")

    # Annotate each point with rule info
    for i in range(len(sorted_rules)):
        plt.annotate(f"{antecedents[i]} -> {consequents[i]}", (lifts_jittered[i], leverages_jittered[i]), fontsize=9, ha='right')

    plt.grid(True)
    plt.show()

    # Print rule details separately
    print(f"Top {n} Strongest Rules:")
    print(f"{'Antecedent':<50} {'Consequent':<50} {'Lift':<10} {'Leverage':<10} {'Conviction':<10}")
    print("="*120)
    for i in range(n):
        print(f"{antecedents[i]:<50} {consequents[i]:<50} {lifts[i]:<10.4f} {leverages[i]:<10.4f} {convictions[i]:<10.4f}")

# Example usage
plot_top_n_strongest_rules(rules, transactions, n=5)

"""# Part - II"""

# prompt: write code to get unique items in transactions

def get_unique_items(transactions):
  """
  Finds all unique items present in a list of transactions.

  Args:
    transactions: A list of transactions, where each transaction is a list or set of items.

  Returns:
    A set of unique items.
  """
  unique_items = set()
  for transaction in transactions:
    for item in transaction:
      unique_items.add(item)
  return unique_items



unique_items = get_unique_items(transactions)
print(unique_items)  # Output: {1, 2, 3, 4, 5}

category_support_map = {
    # High-Frequency Essentials
    'whole milk': 0.02, 'yogurt': 0.02, 'soda': 0.02, 'root vegetables': 0.02,
    'rolls/buns': 0.02, 'tropical fruit': 0.02, 'domestic eggs': 0.02, 'other vegetables': 0.02,

    # Moderate-Frequency Household Items
    'detergent': 0.03, 'kitchen towels': 0.03, 'toilet cleaner': 0.03, 'cling film/bags': 0.03,
    'softener': 0.03, 'napkins': 0.03, 'soap': 0.03, 'bathroom cleaner': 0.03,

    # Low-Frequency Specialty/Seasonal Products
    'specialty cheese': 0.05, 'red/blush wine': 0.05, 'white wine': 0.05, 'sparkling wine': 0.05,
    'liquor': 0.05, 'prosecco': 0.05, 'brandy': 0.05, 'rum': 0.05,

    # Promotion-Focused Items
    'snack products': 0.01, 'candy': 0.01, 'frozen dessert': 0.01, 'pastry': 0.01,
    'ice cream': 0.01, 'popcorn': 0.01, 'specialty chocolate': 0.01, 'salty snack': 0.01,

    # General Groceries (defaults to mid-range)
    'canned beer': 0.03, 'bottled water': 0.03, 'processed cheese': 0.03, 'cream cheese': 0.03,
    'fruit/vegetable juice': 0.03, 'coffee': 0.03, 'canned fish': 0.03, 'tea': 0.03,

    # Catch-all for items not specifically categorized
    'default': 0.04
}

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import combinations, chain
from collections import defaultdict
import numpy as np

# Function to apply dynamic minimum support with default fallback
def get_dynamic_support(item, category_support_map):
    return category_support_map.get(item, category_support_map['default'])

def apriori(transactions, min_support, category_support_map=None):
    item_counts = defaultdict(int)
    total_transactions = len(transactions)

    # Step 1: Generate 1-itemsets
    for transaction in transactions:
        for item in transaction:
            item_counts[frozenset([item])] += 1

    # Apply dynamic minimum support based on item category, if provided
    frequent_itemsets = {itemset: count / total_transactions
                         for itemset, count in item_counts.items()
                         if count / total_transactions >= (get_dynamic_support(list(itemset)[0], category_support_map) if category_support_map else min_support)}

    k = 2
    current_itemsets = frequent_itemsets

    # Step 2: Generate larger itemsets
    while current_itemsets:
        candidate_itemsets = defaultdict(int)

        # Generate candidate itemsets of size k
        itemsets_list = list(current_itemsets.keys())
        for i in range(len(itemsets_list)):
            for j in range(i + 1, len(itemsets_list)):
                candidate = itemsets_list[i] | itemsets_list[j]
                if len(candidate) == k:
                    # Count candidate support
                    candidate_count = sum(1 for transaction in transactions if candidate.issubset(transaction))
                    if candidate_count / total_transactions >= min_support:
                        candidate_itemsets[candidate] = candidate_count / total_transactions

        # Update frequent itemsets
        current_itemsets = {itemset: support for itemset, support in candidate_itemsets.items()}
        frequent_itemsets.update(current_itemsets)
        k += 1

    return frequent_itemsets

# Profitability mapping for various items
profitability_map = {
    # High Profit
    'specialty cheese': 0.09, 'red/blush wine': 0.09, 'white wine': 0.09, 'sparkling wine': 0.09,
    'liquor': 0.09, 'prosecco': 0.09, 'brandy': 0.09, 'rum': 0.09, 'specialty chocolate': 0.09,

    # Moderate Profit
    'whole milk': 0.06, 'yogurt': 0.06, 'fruit/vegetable juice': 0.06, 'bottled water': 0.06,
    'domestic eggs': 0.06, 'root vegetables': 0.06, 'detergent': 0.06, 'coffee': 0.06,
    'canned fish': 0.06, 'tea': 0.06, 'beef': 0.06, 'pork': 0.06,

    # Low Profit
    'rolls/buns': 0.03, 'brown bread': 0.03, 'white bread': 0.03, 'potato products': 0.03,
    'canned beer': 0.03, 'napkins': 0.03, 'cling film/bags': 0.03, 'bathroom cleaner': 0.03,
    'canned vegetables': 0.03, 'salt': 0.03, 'margarine': 0.03, 'soda': 0.03,

    # Promotion-Focused
    'snack products': 0.05, 'candy': 0.05, 'frozen dessert': 0.05, 'pastry': 0.05,
    'ice cream': 0.05, 'popcorn': 0.05, 'specialty bar': 0.05, 'salty snack': 0.05,
    'chocolate': 0.05, 'soft drinks': 0.05,

    # General Groceries (defaults to mid-range)
    'processed cheese': 0.05, 'cream cheese': 0.05, 'jam': 0.05, 'butter': 0.05,
    'rice': 0.05, 'sausage': 0.05,

    # Catch-all for items not specifically categorized
    'default': 0.04
}

def get_profitability(item, profitability_map):
    """Retrieve the profitability of an item from the profitability map.

    Args:
        item (str): The item for which to retrieve profitability.
        profitability_map (dict): The mapping of items to profitability values.

    Returns:
        float: The profitability of the item, or the default value if not found.
    """
    return profitability_map.get(item, profitability_map['default'])

import pandas as pd

def generate_rules(frequent_itemsets, min_confidence, transactions, profitability_map):
    """Generate association rules from frequent itemsets with profitability weighting.

    Args:
        frequent_itemsets (list): List of frequent itemsets.
        min_confidence (float): Minimum confidence threshold for rules.
        transactions (list): List of transactions.
        profitability_map (dict): The mapping of items to profitability values.

    Returns:
        pd.DataFrame: DataFrame of generated rules with support, confidence, and profitability.
    """
    rules = []

    for itemset in frequent_itemsets:
        if len(itemset) < 2:
            continue  # Skip itemsets with less than 2 items

        # Generate all possible rules from the itemset
        for item in itemset:
            antecedent = [i for i in itemset if i != item]
            support = get_support(antecedent + [item], transactions)  # Calculate support for the rule
            antecedent_support = get_support(antecedent, transactions)

            # Calculate confidence only if antecedent support is greater than 0
            confidence = support / antecedent_support if antecedent_support > 0 else 0

            # Add rule only if it meets the confidence threshold
            if confidence >= min_confidence:
                # Get profitability values for the itemset
                profitability_values = [get_profitability(i, profitability_map) for i in itemset]
                average_profitability = sum(profitability_values) / len(profitability_values)

                rules.append({
                    'Antecedent': antecedent,
                    'Consequent': item,
                    'Support': support,
                    'Confidence': confidence,
                    'Profitability': average_profitability  # Add profitability to the rule
                })

    # Convert rules to DataFrame
    if rules:  # Only create a DataFrame if there are rules
        rules_df = pd.DataFrame(rules)
    else:
        rules_df = pd.DataFrame(columns=['Antecedent', 'Consequent', 'Support', 'Confidence', 'Profitability'])

    # Ensure Lift is calculated
    if not rules_df.empty and 'Lift' not in rules_df.columns:
        rules_df['Lift'] = (
            rules_df['Confidence'] / (rules_df['Support'] / len(transactions))
        )

    return rules_df

def get_support(itemset, transactions):
    """Calculate the support for a given itemset."""
    count = sum(1 for transaction in transactions if set(itemset).issubset(set(transaction)))
    return count / len(transactions) if len(transactions) > 0 else 0

# Visualization for top-N frequent itemsets
def plot_top_frequent_itemsets(frequent_itemsets, N=10):
    top_itemsets = sorted(frequent_itemsets.items(), key=lambda x: x[1], reverse=True)[:N]
    items, supports = zip(*top_itemsets)
    items = [' & '.join(list(item)) for item in items]

    plt.figure(figsize=(10, 6))
    sns.barplot(x=supports, y=items, palette="viridis")
    plt.xlabel("Support")
    plt.ylabel("Frequent Itemsets")
    plt.title(f"Top-{N} Frequent Itemsets")
    plt.show()

# Visualization for top-N strongest rules based on Lift
def plot_top_strongest_rule(rules_df, N=10):
    top_rules = rules_df.sort_values(by="Lift", ascending=False).head(N)
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x="Confidence", y="Lift", size="Support", data=top_rules, hue="Lift", palette="coolwarm", legend=False)
    plt.xlabel("Confidence")
    plt.ylabel("Lift")
    plt.title(f"Top-{N} Strongest Rules (by Lift)")
    plt.show()

# Example usage with user inputs
min_support = 0.3
min_confidence =float(input("Enter minimum confidence threshold (as a decimal): "))

# Generate frequent itemsets with dynamic support thresholds
frequent_itemsets = apriori(transactions, min_support, category_support_map=category_support_map)

# Generate association rules with profitability weighting
rules_df = generate_rules(
    frequent_itemsets,     # List of frequent itemsets
    min_confidence,       # Minimum confidence threshold
    transactions,         # List of transactions
    profitability_map     # Profitability mapping
)


# Ensure Lift is calculated in rules_df
if 'Lift' not in rules_df.columns:
    rules_df['Lift'] = (
        rules_df['Confidence'] / (rules_df['Support'] / len(transactions))  # Update to use 'Support' instead of 'RHS Support'
    )

# Plot top-N frequent itemsets
plot_top_frequent_itemsets(frequent_itemsets, N=10)

# Plot top-N strongest rules based on lift and profitability
plot_top_n_strongest_rules(rules, transactions)